# DDPM-From-Scratch-with-MNIST

Project completed on May 3, 2025.

## Project Overview

This project explores denoising diffusion models by training various UNet-based architectures to generate handwritten digits from noise. The models are trained and evaluated on the **MNIST** dataset, progressively improving generation quality by learning to reverse a diffusion (noising) process.
The final models include both time-conditioned and class-conditioned versions of DDPMs, along with visualization of generation steps.

The primary goal was to **understand the diffusion process**, implement it step-by-step in PyTorch, and generate interpretable results using GIFs and sampled outputs.

## What This Project Includes

1.	**Single-Step Denoising UNet**:
* A UNet model trained to denoise a noisy MNIST image with fixed noise level (σ=0.5).
* Visualizations of denoising results and out-of-distribution noise testing.

2. **Time-Conditioned UNet (DDPM)**:
* Implements full DDPM training with time embeddings injected into the UNet.
* Samples generated by iteratively denoising a noise image over 300 timesteps.

3. **Class-Conditioned DDPM**:
* Adds class-conditioning to guide the generation of specific digits (0–9).
* Implements classifier-free guidance for more controllable outputs.

4. **Training Visualizations & GIFs**:
* Model outputs tracked at multiple epochs (1, 5, 10, 15, 20).
* GIFs show the full reverse diffusion process frame by frame.

## Results Summary

1. **Time-conditioned DDPM**
* Epoch 1:
https://github.com/user-attachments/assets/9c5237d4-2457-4f12-9613-6787ac94cc06



* Epoch 20:
https://github.com/user-attachments/assets/1b3e4ee9-84f4-4742-9f28-d1c72f4d7e81


2. **Class-conditioned DDPM**
* Epoch 1:


* Epoch 20:


As shown in the samples, class-conditioning significantly improves the denoising process, enabling clearer and more controlled digit generation by epoch 20.

## Repository Contents
* `ddpm-implementation.ipynb` -- Main Jupyter Notebook with full implementation. 
* `Report.pdf` -- Summary of training progress and results.
